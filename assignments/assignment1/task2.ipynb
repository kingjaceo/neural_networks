{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e92e00",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f8c5c",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* Resize input images to 64×64×3 or 128×128×3, depending on available computational resources\n",
    "* Normalize both training and testing image sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420c777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1440 images of shape (128, 128, 3)\n",
      "Classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Pixel range before normalization: [0, 255]\n",
      "Pixel range after normalization: [0.00, 1.00]\n",
      "\n",
      "Training set: 1152 samples\n",
      "Testing set:  288 samples\n",
      "\n",
      "Train loader: 1036 samples, 33 batches\n",
      "Val loader:   116 samples, 4 batches\n",
      "Test loader:  288 samples, 9 batches\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 32\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "DATA_DIR = \"coil-20-proc/coil-20-proc\"\n",
    "\n",
    "\n",
    "def parse_class_label_from_filename(filename):\n",
    "    object_prefix = filename.split(\"__\")[0]\n",
    "    one_indexed_label = int(object_prefix.replace(\"obj\", \"\"))\n",
    "    return one_indexed_label - 1\n",
    "\n",
    "\n",
    "def load_coil20_images_and_labels(data_dir):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    for filename in sorted(os.listdir(data_dir)):\n",
    "        if not filename.endswith(\".png\"):\n",
    "            continue\n",
    "        label = parse_class_label_from_filename(filename)\n",
    "        grayscale_image = Image.open(os.path.join(data_dir, filename))\n",
    "        rgb_image = grayscale_image.convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        all_images.append(np.array(rgb_image))\n",
    "        all_labels.append(label)\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "\n",
    "def normalize_pixel_values(images):\n",
    "    return images.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def numpy_to_channels_first_tensor(images):\n",
    "    return torch.tensor(images).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class COIL20Dataset(Dataset):\n",
    "    def __init__(self, image_tensors, label_tensors):\n",
    "        self.image_tensors = image_tensors\n",
    "        self.label_tensors = label_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_tensors[idx], self.label_tensors[idx]\n",
    "\n",
    "\n",
    "raw_images, labels = load_coil20_images_and_labels(DATA_DIR)\n",
    "print(f\"Loaded {raw_images.shape[0]} images of shape {raw_images.shape[1:]}\")\n",
    "print(f\"Classes: {np.unique(labels)}\")\n",
    "print(f\"Pixel range before normalization: [{raw_images.min()}, {raw_images.max()}]\")\n",
    "\n",
    "normalized_images = normalize_pixel_values(raw_images)\n",
    "print(f\"Pixel range after normalization: [{normalized_images.min():.2f}, {normalized_images.max():.2f}]\")\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    normalized_images, labels, test_size=TEST_SPLIT, random_state=SEED, stratify=labels\n",
    ")\n",
    "print(f\"\\nTraining set: {train_images.shape[0]} samples\")\n",
    "print(f\"Testing set:  {test_images.shape[0]} samples\")\n",
    "\n",
    "train_image_tensors = numpy_to_channels_first_tensor(train_images)\n",
    "test_image_tensors = numpy_to_channels_first_tensor(test_images)\n",
    "train_label_tensors = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_label_tensors = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = COIL20Dataset(train_image_tensors, train_label_tensors)\n",
    "test_dataset = COIL20Dataset(test_image_tensors, test_label_tensors)\n",
    "\n",
    "train_subset_size = int((1 - VALIDATION_SPLIT) * len(train_dataset))\n",
    "val_subset_size = len(train_dataset) - train_subset_size\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, [train_subset_size, val_subset_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain loader: {len(train_subset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"Val loader:   {len(val_subset)} samples, {len(val_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff3de3",
   "metadata": {},
   "source": [
    "### Activation Function Design and Comparison\n",
    "* Use an AI-assistive system to design a novel activation function (reference: ReLU, Swish, GELU, TeLU, etc.)\n",
    "* Integrate the proposed activation function into the DCNN\n",
    "* Compare its performance against standard activation functions such as ReLU and GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07a82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a5fb47",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "Train the model using the following optimization methods:\n",
    "* Stochastic Gradient Descent (SGD) with momentum\n",
    "* Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78d5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf4aeaeb",
   "metadata": {},
   "source": [
    "### Training Analysis\n",
    "Plot training and validation loss curves to demonstrate learning behavior and convergence during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbad63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "408d4198",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Evaluate the trained model on the test dataset using the following performance metrics:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "* Confusion matrix\n",
    "* ROC curve\n",
    "* Precision–Recall curve\n",
    "\n",
    "Present overall performance without thresholding, then also report metrics for predictions with confidence > 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c60c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d79eae99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
